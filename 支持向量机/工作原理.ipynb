{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "偏置项表示为$b$, 特征权重向量表示为$w$, 同时输入特征向量中不添加偏置特征。\n",
    "\n",
    "1. 决策函数和预测：\n",
    "\n",
    "    线性SVM分类器通过简单地计算决策函数$w^{T}x+b=w_{1}x_{1}+...+w_{n}x_{n}+b$来预测新实例$x$的分类，若为正则预测类别是正类(1), 否则预测其为负类(0).\n",
    "\n",
    "2. 训练目标：\n",
    "\n",
    "    硬间隔线性SVM分类器的目标： $minimize \\frac{1}{2}w^{T}w$   $\\quad \\quad$    $s.t. \\quad t^{(i)}(w^{T}x^{(i)}+b) \\geq 1$ ,  $i=1,2,...m$\n",
    "\n",
    "    引入松弛变量$\\zeta^{(i)} \\geq 0$,  $\\zeta^{(i)}$衡量的是第$i$个实例多大程度上允许间隔违例。 超参数$C$允许我们在“使松弛变量越小越好从而减小间隔违例”和“使$\\frac{1}{2}w^{T}w$最小化以增大间隔”这两个目标之间权衡\n",
    "\n",
    "    软间隔线性SVM分类器的目标： $\\min_{w,b,\\zeta} \\frac{1}{2}w^{T}w+C\\sum^{m}_{i=1}\\zeta^{(i)}$   $\\quad \\quad$    $s.t. \\quad t^{(i)}(w^{T}x^{(i)}+b) \\geq 1-\\zeta^{(i)}$和$\\zeta^{(i)}\\geq 0$ ,  $i=1,2,...m$\n",
    "\n",
    "3. 二次规划：\n",
    "\n",
    "    一般形式$\\min_{p}\\frac{1}{2}p^{T}Hp+f^{T}p$,  使得$Ap\\leq b$\n",
    "\n",
    "4. 对偶问题：\n",
    "\n",
    "    线性SVM目标的对偶形式：$\\min_{\\alpha} \\frac{1}{2}\\sum^{m}_{i=1}\\sum^{m}_{j=1} \\alpha^{(i)}\\alpha^{(j)}t^{(i)}t^{(j)}x^{(i)T}x^{(j)}-\\sum^{m}_{i=1}\\alpha^{(i)}$    $\\quad \\quad$    $s.t. \\quad \\alpha^{(i)}\\geq 0$ ,  $i=1,2,...m$\n",
    "\n",
    "    从对偶问题到原始问题：$\\hat{w}=\\sum^{m}_{i=1}\\hat{\\alpha^{(i)}}t^{(i)}x^{(i)}$ $\\quad\\quad$ $\\hat{b}=\\frac{1}{n_{s}}\\sum^{m}_{i=1,\\hat{\\alpha^{(i)}}\\geq 0}(t^{(i)}-\\hat{w^{T}}x^{(i)})$\n",
    "\n",
    "    当训练实例的数量小于特征数量时，解决对偶问题比原始问题更快速\n",
    "\n",
    "5. 内核化SVM：\n",
    "\n",
    "    二阶多项式映射：$\\phi(\\boldsymbol{x}) = \\phi\\left( \\begin{pmatrix} x_1 \\\\ x_2 \\end{pmatrix} \\right) = \\begin{pmatrix} x_1^2 \\\\ \\sqrt{2} x_1 x_2 \\\\ x_2^2 \\end{pmatrix}$\n",
    "\n",
    "    二维多项式映射的核方法：$\\phi(\\boldsymbol{a})^{\\mathrm{T}} \\phi(\\boldsymbol{b}) = \\begin{pmatrix} a_1^2 \\\\ \\sqrt{2} a_1 a_2 \\\\ a_2^2 \\end{pmatrix}^{\\mathrm{T}} \\begin{pmatrix} b_1^2 \\\\ \\sqrt{2} b_1 b_2 \\\\ b_2^2 \\end{pmatrix} = a_1^2 b_1^2 + 2 a_1 b_1 a_2 b_2 + a_2^2 b_2^2= (a_1 b_1 + a_2 b_2)^2 = \\left( \\begin{pmatrix} a_1 \\\\ a_2 \\end{pmatrix}^{\\mathrm{T}} \\begin{pmatrix} b_1 \\\\ b_2 \\end{pmatrix} \\right)^2 = (\\boldsymbol{a}^{\\mathrm{T}} \\boldsymbol{b})^2$\n",
    "\n",
    "    核是能够仅基于原始向量$a$和$b$来计算点积$\\phi(a)^{T}\\phi(b)$的函数，它不需要计算转换函数。\n",
    "\n",
    "    常用的核函数：\n",
    "\n",
    "   $\\text{线性:} \\quad K(\\boldsymbol{a}, \\boldsymbol{b}) = \\boldsymbol{a}^{\\mathrm{T}} \\boldsymbol{b}$\n",
    "   $\\text{多项式:} \\quad K(\\boldsymbol{a}, \\boldsymbol{b}) = (\\gamma \\boldsymbol{a}^{\\mathrm{T}} \\boldsymbol{b} + r)^d$\n",
    "   $\\text{高斯 RBF:} \\quad K(\\boldsymbol{a}, \\boldsymbol{b}) = \\exp(-\\gamma \\| \\boldsymbol{a} - \\boldsymbol{b} \\|^2)$\n",
    "   $\\text{Sigmoid:} \\quad K(\\boldsymbol{a}, \\boldsymbol{b}) = \\tanh(\\gamma \\boldsymbol{a}^{\\mathrm{T}} \\boldsymbol{b} + r)$\n",
    "\n",
    "    使用核化SVM做出预测：$h_{\\hat{\\boldsymbol{w}}, \\hat{b}}(\\phi(\\boldsymbol{x}^{(n)})) = \\hat{\\boldsymbol{w}}^{\\mathrm{T}} \\phi(\\boldsymbol{x}^{(n)}) + \\hat{b} = \\left( \\sum_{i=1}^{m} \\hat{\\alpha}^{(i)} t^{(i)} \\phi(\\boldsymbol{x}^{(i)}) \\right)^{\\mathrm{T}} \\phi(\\boldsymbol{x}^{(n)}) + \\hat{b}= \\sum_{i=1}^{m} \\hat{\\alpha}^{(i)} t^{(i)} (\\phi(\\boldsymbol{x}^{(i)})^{\\mathrm{T}} \\phi(\\boldsymbol{x}^{(n)})) + \\hat{b}= \\sum_{\\substack{i=1 \\\\ \\hat{\\alpha}^{(i)} > 0}}^{m} \\hat{\\alpha}^{(i)} t^{(i)} K(\\boldsymbol{x}^{(i)}, \\boldsymbol{x}^{(n)}) + \\hat{b}$\n",
    "\n",
    "    使用核方法计算偏置项：$\\hat{b} = \\frac{1}{n_s} \\sum_{\\substack{i=1 \\\\ \\hat{\\alpha}^{(i)} > 0}}^{m} \\left( t^{(i)} - \\hat{\\boldsymbol{w}}^{\\mathrm{T}} \\phi(\\boldsymbol{x}^{(i)}) \\right)\n",
    "= \\frac{1}{n_s} \\sum_{\\substack{i=1 \\\\ \\hat{\\alpha}^{(i)} > 0}}^{m} \\left( t^{(i)} - \\left( \\sum_{j=1}^{m} \\hat{\\alpha}^{(j)} t^{(j)} \\phi(\\boldsymbol{x}^{(j)}) \\right)^{\\mathrm{T}} \\phi(\\boldsymbol{x}^{(i)}) \\right)= \\frac{1}{n_s} \\sum_{\\substack{i=1 \\\\ \\hat{\\alpha}^{(i)} > 0}}^{m} \\left( t^{(i)} - \\sum_{\\substack{j=1 \\\\ \\hat{\\alpha}^{(j)} > 0}}^{m} \\hat{\\alpha}^{(j)} t^{(j)} K(\\boldsymbol{x}^{(i)}, \\boldsymbol{x}^{(j)}) \\right)$\n",
    "\n",
    "\n",
    "\n",
    "6. 在线SVM：\n",
    "\n",
    "   线性SVM分类器成本函数：$J(\\boldsymbol{w}, b) = \\frac{1}{2} \\boldsymbol{w}^{\\mathrm{T}} \\boldsymbol{w} + C \\sum_{i=1}^{m} \\max\\left(0, 1 - t^{(i)} (\\boldsymbol{w}^{\\mathrm{T}} \\boldsymbol{x}^{(i)} + b)\\right)$\n",
    "\n",
    "   Hinge损失函数: $\\max{(0, 1-t)}$"
   ],
   "id": "ceaa301694f8debf"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
